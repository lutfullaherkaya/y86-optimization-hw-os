#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
    # corrupt all the unused registers to prevent assumptions
    irmovq $0x5710331, %rax
    irmovq $0x5710331, %rbx
    irmovq $0x5710331, %rcx
    irmovq $0x5710331, %rbp
    irmovq $0x5710331, %r8
    irmovq $0x5710331, %r9
    irmovq $0x5710331, %r10
    irmovq $0x5710331, %r11
    irmovq $0x5710331, %r12
    irmovq $0x5710331, %r13
    irmovq $0x5710331, %r14
	call absrev		 
	halt			# should halt with abs sum in %rax
StartFun:
#/* $begin absrev-ys */
##################################################################
# absrev.ys - Reverse a src block of len words to dst.
# Return the sum of absolute values of words contained in src.
#
# Lutfullah Erkaya 2448363
# todo: Describe how and why you modified the baseline code.
# beginning: 17.83 cpe
# 17.53 after multiplying %rcx by two instead of adding one by one
# 15.46 after using leaq for constant substraction and addition
# 14.42 after substracting negative value from sum instead of taking absolute value
# 13.94 after updating loop variables before the sum addition and  using duplicate code instead of jumping to the Negative branch.

##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
absrev:
##################################################################
# You can modify this portion
    # Loop header
    xorq %rax,%rax    	# sum = 0;

    
    # all this for dst_rev = dst + len - 1
    xorq %rcx, %rcx     	# zero rcx
    addq %rdx, %rcx     	# rcx = len
    addq %rcx, %rcx     	# rcx = 2*len
    addq %rcx, %rcx     	# rcx = 4*len
    addq %rcx, %rcx     	# rcx = 8*len
    
    leaq $-8(%rcx), %rcx    # rcx -= 8
    addq %rsi, %rcx     	# add dst. finally got dst_rev = dst + len - 1
	

    andq %rdx,%rdx    		# len <= 0?
    jle UnrolledDone            	# if so, goto Done:
UnrolledLoop:    
    mrmovq (%rdi), %r10 	# read val from src...
    rmmovq %r10, (%rcx) 	# ...and store it to dst
    mrmovq $8(%rdi), %r11 	# read val from src+1...
    rmmovq %r11, $-8(%rcx) 	# ...and store it to dst-1

	leaq $-2(%rdx), %rdx	# len -= 2
    leaq $16(%rdi), %rdi	# src += 2
	leaq $-16(%rcx), %rcx	# dst_rev -= 2

	addq %r11, %r10			# sum the values of unrolled iteration
    andq %r10, %r10    		# val >= 0?
    jge UnrolledPositive	# if so, skip negating
    subq %r10, %rax      	# sum -= negativevalue


	# duplicate code for avoiding the unneccesary jump to Negative
    icmpq $1, %rdx    		# len > 1?
    jg UnrolledLoop			# if so, goto UnrolledLoop:
	jmp UnrolledDone
UnrolledPositive:
    addq %r10, %rax     	# sum += positivevalue

    icmpq $1, %rdx    		# len > 1?
    jg UnrolledLoop			# if so, goto UnrolledLoop:
UnrolledDone:

    andq %rdx,%rdx    		# len <= 0?
    jle Done            	# if so, goto Done:
Loop:
	mrmovq (%rdi), %r10 	# read val from src...
    rmmovq %r10, (%rcx) 	# ...and store it to dst

	leaq $-1(%rdx), %rdx	# len -= 1
    leaq $8(%rdi), %rdi		# src += 1
	leaq $-8(%rcx), %rcx	# dst_rev -= 1

    andq %r10, %r10    		# val >= 0?
    jge Positive        	# if so, skip negating
    subq %r10, %rax      	# sum -= negativevalue


	# duplicate code for avoiding the unneccesary jump to Negative
    andq %rdx, %rdx    		# len > 0?
    jg Loop             	# if so, goto Loop:
	jmp Done
Positive:
    addq %r10, %rax     	# sum += positivevalue

    andq %rdx,%rdx    		# len > 0?
    jg Loop             	# if so, goto Loop:
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
    ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end absrev-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad -3
	.quad 4
	.quad -5
	.quad -6
	.quad 7
	.quad 8
	.quad 9
	.quad -10
	.quad 11
	.quad 12
	.quad 13
	.quad 14
	.quad -15
	.quad -16
	.quad -17
	.quad 18
	.quad 19
	.quad 20
	.quad -21
	.quad -22
	.quad 23
	.quad -24
	.quad 25
	.quad -26
	.quad -27
	.quad -28
	.quad 29
	.quad -30
	.quad -31
	.quad 32
	.quad 33
	.quad 34
	.quad -35
	.quad 36
	.quad 37
	.quad 38
	.quad 39
	.quad 40
	.quad 41
	.quad -42
	.quad -43
	.quad -44
	.quad -45
	.quad 46
	.quad -47
	.quad 48
	.quad -49
	.quad -50
	.quad 51
	.quad 52
	.quad 53
	.quad -54
	.quad -55
	.quad 56
	.quad -57
	.quad 58
	.quad -59
	.quad -60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
